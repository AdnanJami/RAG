{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#api=\"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\"\n",
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(groq_api_key=api, model_name=\"llama-3.2-3b-preview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Force is a fundamental concept in physics that describes a push or pull that causes an object to change its state of motion or shape. It is a measure of the interaction between two objects, and it depends on the mass of the objects, the distance between them, and the acceleration they produce.\\n\\nThere are several types of forces, including:\\n\\n1. **Gravitational force**: The force of attraction between two objects with mass, such as the Earth and an object on its surface.\\n2. **Frictional force**: The force that opposes motion between two surfaces that are in contact, such as the force that prevents a car from slipping on a wet road.\\n3. **Normal force**: The force exerted by a surface on an object in contact with it, such as the force that presses a book onto a table.\\n4. **Tensile force**: The force that stretches or pulls apart two objects, such as the force that holds a rope or a piece of string.\\n5. **Compressive force**: The force that compresses or squeezes two objects together, such as the force that presses two blocks of wood together.\\n6. **Electric force**: The force that acts between charged particles, such as the force that holds an electric charge onto a surface.\\n7. **Magnetic force**: The force that acts between magnetic fields, such as the force that holds a magnet onto a metal surface.\\n\\nThe law of action and reaction states that for every action, there is an equal and opposite reaction. This means that when object A exerts a force on object B, object B will exert an equal and opposite force on object A.\\n\\nThe force can be described using the following formulas:\\n\\n1. **Newton's second law**: F = ma (Force = mass x acceleration)\\n2. **Force and distance**: F = k x x (Force = spring constant x displacement)\\n\\nThe understanding of force is essential in various fields, such as physics, engineering, and mechanics, to design and build machines, mechanisms, and structures that can withstand various forces and stress.\\n\\nSome of the applications of force include:\\n\\n1. **Mechanical engineering**: designing machines, mechanisms, and devices that can withstand various forces.\\n2. **Civil engineering**: designing buildings, bridges, and other structures that can withstand various forces.\\n3. **Biomechanics**: studying the forces that act on living organisms and their tissues.\\n4. **Aerospace engineering**: designing aircraft, spacecraft, and missiles that can withstand various forces.\\n5. **Materials science**: studying the properties of materials and how they respond to various forces.\\n\\nIn conclusion, force is a fundamental concept in physics that describes the interaction between objects and their effect on motion and shape. Understanding force is essential in various fields to design and build machines, mechanisms, and structures that can withstand various forces and stress.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 573, 'prompt_tokens': 38, 'total_tokens': 611, 'completion_time': 0.335346669, 'prompt_time': 0.00622981, 'queue_time': 0.008191579000000001, 'total_time': 0.341576479}, 'model_name': 'llama-3.2-3b-preview', 'system_fingerprint': 'fp_a926bfdce1', 'finish_reason': 'stop', 'logprobs': None}, id='run-43d4f906-df59-4e8b-927c-4be41193e38d-0', usage_metadata={'input_tokens': 38, 'output_tokens': 573, 'total_tokens': 611})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"what is force\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\importlib\\__init__.py:126: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  return _bootstrap._gcd_import(name[level:], package, level)\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_20120\\2625692788.py:14: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 0.4. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "from datetime import datetime\n",
    "import re\n",
    "import chromadb\n",
    "import ollama\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "oembed = OllamaEmbeddings(base_url=\"http://localhost:11434\", model=\"nomic-embed-text\")\n",
    "collection_name=\"phydb\"  ##phydbllama3\n",
    "\n",
    "\n",
    "persist_directory=\"database\"\n",
    "vectorstore = Chroma(\n",
    "     collection_name=collection_name,\n",
    "    embedding_function=oembed,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def llmRagQuery(prompt):\n",
    "    model = ollama.generate(\n",
    "        model=\"llama3.2\",\n",
    "        system=\"\"\"\n",
    "        you are a rag assistant who will rewrite the question/prompt given by the user.\n",
    "        you will rewrite 3 more times.\n",
    "        the rewritten prompts/questions has to be exactly the same topic as that of the user's.\n",
    "        it can't be too advanced nor too simplistic compared to the user's\n",
    "        you will not include anything else in your answer.\n",
    "        list the rewritten questions/prompts in an ordered list\n",
    "\n",
    "        \"\"\",\n",
    "        prompt=prompt,\n",
    "    )\n",
    "\n",
    "    newPrompts = prompt + \" \"\n",
    "    llmPrompts = re.compile(r\"([0-9 .]+)(.+)\")\n",
    "    llmPrompts = re.finditer(llmPrompts,model[\"response\"])\n",
    "\n",
    "    for prompts in llmPrompts:\n",
    "        newPrompts += prompts.group(2) + \" \"\n",
    "    result = retriever.invoke(newPrompts)\n",
    "    str = \" \"\n",
    "    for i in result:\n",
    "        str+= i.page_content\n",
    "    return str\n",
    "modelMini = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "def check_Meaning(old_response, new_response):\n",
    "    # embed the texts\n",
    "    embeddings = modelMini.encode([old_response, new_response])\n",
    "\n",
    "\n",
    "    similarity = util.cos_sim(embeddings[0], embeddings[1])\n",
    "    \n",
    "    if similarity.item()*100 > 50:\n",
    "        return new_response\n",
    "\n",
    "    else:\n",
    "        return old_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "path ='dataset/fact/allfact.json'\n",
    "with open(path) as f:\n",
    "  json_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SaveDataset(data, path=None, fileName=\"syntheticDatafactall\"):\n",
    "  if path is None:\n",
    "      filePath = f\"{fileName}.json\"\n",
    "  else:\n",
    "      filePath = f\"{path}/{fileName}.json\"\n",
    "  with open(filePath, 'a') as f:\n",
    "      json.dump(data, f, indent=4)\n",
    "  print(\"Saved Successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/864 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "100%|██████████| 864/864 [4:16:50<00:00, 17.84s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Successfully!\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "p=[]\n",
    "for i in tqdm(json_data):\n",
    "    message = i['question']\n",
    "    query = llmRagQuery(message)\n",
    "    tmp = check_Meaning(message,query)\n",
    "    context = \" \"\n",
    "    if tmp == message:\n",
    "        tmp = \"None \"\n",
    "\n",
    "    else:\n",
    "            context = tmp\n",
    "    system =\"\"\"You are an A.I. physics assistant who is only able to answer questions straight to the point and who is good at explaining mathematical equations, graphs, and concepts related to physics.\n",
    "                you are going to answer in a way that is easier for 9th grade students to understand.\n",
    "                If you do not know something, then say so; otherwise, answer concisely and accurately.\n",
    "                Absolutely don’t repeat system context, or userPrompt in your response.\n",
    "                if the user provides context, then use that context to answer the question but it has to be relevant to the user prompt otherwise say you dont know.\n",
    "            \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "        \"Context\": {context},\n",
    "        \"UserPrompt\": {message}\n",
    "    \"\"\"\n",
    "\n",
    "    response = ollama.generate(\n",
    "    model=\"llama3.2\",\n",
    "    system=system,\n",
    "    prompt=prompt\n",
    "\n",
    "    )\n",
    "    j ={\n",
    "    \"question\":message,\n",
    "    \"answer\":response['response'],\n",
    "    \"referenceAnswer\": i['referenceAnswer'],\n",
    "    \"referenceContext\": i['referenceContext'],\n",
    "    \"context\": context\n",
    "    }\n",
    "    p.append(j)\n",
    "SaveDataset(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "\n",
    "# Your OpenRouter API key\n",
    "api_key = \"sk-or-v1-af7953477f9e13e5d40bd26b631f4a6525a0e05d8ffd0bd30f420e077275f08\"\n",
    "\n",
    "# OpenRouter endpoint and model\n",
    "endpoint = \"https://openrouter.ai/nousresearch/hermes-3-llama-3.1-405b:free/api\"\n",
    "# Define the request payload\n",
    "payload = {\n",
    "    \"prompt\": \"Tell me a joke\",\n",
    "    \"max_tokens\": 100\n",
    "}\n",
    "\n",
    "# Set the headers, including the API key\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Make the POST request to OpenRouter\n",
    "response = requests.post(endpoint, json=payload, headers=headers)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Print the response from the model\n",
    "    result = response.json()\n",
    "    print(result[\"text\"])\n",
    "else:\n",
    "    print(f\"Request failed with status code {response.status_code}: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "path ='conversation/chap4.json'\n",
    "with open(path) as f:\n",
    "  json_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=[]\n",
    "for i in json_data:\n",
    "    a = LLMTestCase(\n",
    "            input=i[\"input\"],\n",
    "            actual_output=i[\"actual_output\"] \n",
    "                          )\n",
    "    p.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LLMTestCase(input='When does work happen if I hold a book still in my hand?', actual_output='Great question. Holding a book still, thereâ€™s no displacement, so by the scientific definition, no work is done. But if you moved the book from one place to another, that would count as work. Can you think of examples of work in everyday life?', expected_output=None, context=None, retrieval_context=None, additional_metadata=None, comments=None, tools_called=None, expected_tools=None, reasoning=None, name=None),\n",
       " LLMTestCase(input='Yes, but I thought work happened even if I just held something heavy for a long time.', actual_output=\"Thatâ€™s understandable since it feels like you're expending energy, but in physics, work requires displacement along with force. Imagine pushing a cart or lifting a weight; in those cases, thereâ€™s both force and movement, so we can measure work.\", expected_output=None, context=None, retrieval_context=None, additional_metadata=None, comments=None, tools_called=None, expected_tools=None, reasoning=None, name=None),\n",
       " LLMTestCase(input=\"So, lifting a brick to a higher floor involves more work than lifting it to a lower floor because there's more distance?\", actual_output='Exactly! The work increases with the distance the object is moved. In physics, we calculate work as the product of the force applied and the distance traveled in the direction of that force.', expected_output=None, context=None, retrieval_context=None, additional_metadata=None, comments=None, tools_called=None, expected_tools=None, reasoning=None, name=None),\n",
       " LLMTestCase(input='Got it! So work depends on both force and distance.', actual_output='Right! And this relationship is central to understanding energy transformation, which weâ€™ll explore more in the chapter.', expected_output=None, context=None, retrieval_context=None, additional_metadata=None, comments=None, tools_called=None, expected_tools=None, reasoning=None, name=None),\n",
       " LLMTestCase(input='What exactly is energy in this context?', actual_output='Good question. In physics, energy is the ability to do work. If something has energy, it has the potential to apply force and cause displacement, which we measure as work.', expected_output=None, context=None, retrieval_context=None, additional_metadata=None, comments=None, tools_called=None, expected_tools=None, reasoning=None, name=None),\n",
       " LLMTestCase(input='How many types of energy are there?', actual_output='There are several forms of energy. Common types include kinetic energy (energy of motion}, potential energy (stored energy based on position}, and others like chemical, thermal, and electrical energy.', expected_output=None, context=None, retrieval_context=None, additional_metadata=None, comments=None, tools_called=None, expected_tools=None, reasoning=None, name=None),\n",
       " LLMTestCase(input='Could you give me an example of kinetic energy?', actual_output='Sure! A moving car has kinetic energy because itâ€™s in motion. If it hits an object, it can exert force and do work on that object, like pushing it or causing it to move.', expected_output=None, context=None, retrieval_context=None, additional_metadata=None, comments=None, tools_called=None, expected_tools=None, reasoning=None, name=None),\n",
       " LLMTestCase(input='And what about potential energy?', actual_output='Potential energy is stored energy due to an objectâ€™s position. For example, a book on a shelf has gravitational potential energy because, if it falls, gravity will pull it down, doing work as it moves.', expected_output=None, context=None, retrieval_context=None, additional_metadata=None, comments=None, tools_called=None, expected_tools=None, reasoning=None, name=None),\n",
       " LLMTestCase(input='What happens to the energy of the book when it falls?', actual_output='Great observation! As the book falls, its potential energy transforms into kinetic energy. By the time it hits the ground, most of its initial potential energy has converted into kinetic energy.', expected_output=None, context=None, retrieval_context=None, additional_metadata=None, comments=None, tools_called=None, expected_tools=None, reasoning=None, name=None),\n",
       " LLMTestCase(input='Is that what you mean by energy transformation?', actual_output='Exactly! Energy transformation is the process of energy changing from one form to another, like potential energy turning into kinetic energy when an object falls.', expected_output=None, context=None, retrieval_context=None, additional_metadata=None, comments=None, tools_called=None, expected_tools=None, reasoning=None, name=None),\n",
       " LLMTestCase(input='Does this mean energy is never lost?', actual_output='Yes, in a closed system, energy is conserved. It might change forms, but the total energy remains constant. This principle is known as the law of conservation of energy.', expected_output=None, context=None, retrieval_context=None, additional_metadata=None, comments=None, tools_called=None, expected_tools=None, reasoning=None, name=None),\n",
       " LLMTestCase(input='Whatâ€™s the unit of energy?', actual_output='Energy is measured in joules (J}, the same unit as work, because energy is essentially the capacity to do work.', expected_output=None, context=None, retrieval_context=None, additional_metadata=None, comments=None, tools_called=None, expected_tools=None, reasoning=None, name=None),\n",
       " LLMTestCase(input='How can we calculate work if we know the force and distance?', actual_output='To calculate work, you multiply the force applied by the distance over which it acts, in the direction of the force. So, Work : Force Ã— Distance.', expected_output=None, context=None, retrieval_context=None, additional_metadata=None, comments=None, tools_called=None, expected_tools=None, reasoning=None, name=None),\n",
       " LLMTestCase(input='Does work have direction?', actual_output='No, work is a scalar quantity, meaning it has magnitude but no specific direction, even though it depends on the direction of force and displacement.', expected_output=None, context=None, retrieval_context=None, additional_metadata=None, comments=None, tools_called=None, expected_tools=None, reasoning=None, name=None),\n",
       " LLMTestCase(input='So energy and work are related but different concepts?', actual_output='Yes, they are closely related. Work is a transfer of energy; doing work on an object transfers energy to it, and energy is essentially the capacity to perform work.', expected_output=None, context=None, retrieval_context=None, additional_metadata=None, comments=None, tools_called=None, expected_tools=None, reasoning=None, name=None)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
