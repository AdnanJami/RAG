{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"Physicsnew.pdf\")\n",
    "text_doc = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_split = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap= 100)\n",
    "documents = text_split.split_documents(text_doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "oembed = OllamaEmbeddings(base_url=\"http://localhost:11434\", model=\"nomic-embed-text\")\n",
    "from collections import deque\n",
    "collection_name=\"phydb\"  ##phydbllama3\n",
    "    \n",
    "persist_directory=\"database\"\n",
    "vectorstore = Chroma(\n",
    "     collection_name=collection_name,  \n",
    "    embedding_function=oembed,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' force.force is the consequence of the direct contact between the hand and the body. The examples of contact forces are- frictional force, pulling force and the force created during collision.this force is called gravity.3.2 Nature of force\\n\\nContact force:'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"what is force ?\"\n",
    "result = vectorstore.similarity_search(question)\n",
    "str = \" \"\n",
    "for i in result:\n",
    "        str+= i.page_content\n",
    "\n",
    "str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "#API_KEY=\"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\"\n",
    "client = Groq(api_key=API_KEY)\n",
    "def generate_response(context, query, model=\"llama-3.2-3b-preview\"):\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": f\"Context: {context} Query: {query}\"}],\n",
    "        model=model,\n",
    "        stream=False,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are an A.I. physics assistant who is only able to answer questions straight to the point and who is good at explaining mathematical equations, graphs, and concepts related to physics.\\n                you are going to answer in a way that is easier for 9th grade students to understand.\\n                If you do not know something, then say so; otherwise, answer concisely and accurately.\\n                Absolutely don’t repeat system context, or userPrompt in your response.\\n                if the user provides context, then use that context to answer the question but it has to be relevant to the user prompt otherwise say you dont know.\\n            \\n                \"Context\": \"{str}\",\\n                \"UserPrompt\": \"{question}\"\\n    '"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "api=\"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\"\n",
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(groq_api_key=api, model_name=\"llama-3.2-3b-preview\")\n",
    "question=\"what is force\"\n",
    "def ragi( question):\n",
    "   \n",
    "    result = vectorstore.similarity_search(question)\n",
    "    str = \" \"\n",
    "    for i in result:\n",
    "        str+= i.page_content\n",
    "\n",
    "    \n",
    "    racist=\"\"\"You are an A.I. physics assistant who is only able to answer questions straight to the point and who is good at explaining mathematical equations, graphs, and concepts related to physics.\n",
    "                you are going to answer in a way that is easier for 9th grade students to understand.\n",
    "                If you do not know something, then say so; otherwise, answer concisely and accurately.\n",
    "                Absolutely don’t repeat system context, or userPrompt in your response.\n",
    "                if the user provides context, then use that context to answer the question but it has to be relevant to the user prompt otherwise say you dont know.\n",
    "            \n",
    "                \"Context\": \"{str}\",\n",
    "                \"UserPrompt\": \"{question}\"\n",
    "    \"\"\"\n",
    "    return racist\n",
    "system_prompt =ragi(question)\n",
    "system_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an A.I. physics assistant who is only able to answer questions straight to the point and who is good at explaining mathematical equations, graphs, and concepts related to physics.\n",
      "                you are going to answer in a way that is easier for 9th grade students to understand.\n",
      "                If you do not know something, then say so; otherwise, answer concisely and accurately.\n",
      "                Absolutely don’t repeat system context, or userPrompt in your response.\n",
      "                if the user provides context, then use that context to answer the question but it has to be relevant to the user prompt otherwise say you dont know.\n",
      "            \n",
      "        \"Context\": {str},\n",
      "        \"UserPrompt\": {question}\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_pipeline( question):\n",
    "   \n",
    "    result = vectorstore.similarity_search(question)\n",
    "    str = \" \"\n",
    "    for i in result:\n",
    "        str+= i.page_content\n",
    "\n",
    "    # Step 4: Generate response\n",
    "    response = generate_response(str, query)\n",
    "    return response\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, it appears that the question is about the nature of contact force. \\n\\nTo answer the query: \"Contact force,\" we can rely on the information provided in the documents.\\n\\nAccording to the documents, contact forces are consequences of direct contact between objects. Examples of such forces are:\\n\\n1. Frictional force: The force that opposes the motion between two surfaces in contact.\\n2. Pulling force: A force that attracts or pulls an object towards another object.\\n3. The force created during collision: The force that is exerted when two objects collide.\\n\\nHowever, the documents do not provide a clear and concise definition of what a contact force is.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_pipeline(\"what is force?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The uses of solar cells include:\\n\\n* Supplying electricity to artificial satellites\\n* Operating electronic devices such as pocket calculators, radios, and watches\\n* Heating dwellings in cold countries\\n* Drying crops, fish, vegetables, and other materials\\n* Producing electrical energy for rural areas, houses, or offices\\n* Igniting fires through concentration of sunlight\\n* Cooking food using solar cookers\\n\\nThese uses highlight the potential of solar cells to provide a clean and sustainable source of energy.\\n\\nPage: 81'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(model=\"llama3\")\n",
    "\n",
    "prompt = f\"\"\"\n",
    "system_prompt: you are an AI teaching assistant who is specialized in physics and you dont have to\n",
    " introduce yourself .there will be textContext . \n",
    "If there is context, you will assist me using those or else you give normal feedback like a chatbot.\n",
    "Mention the page number at the end of your response.\n",
    "\n",
    "\n",
    "question: {question}\n",
    "\n",
    "textContext:{query} \n",
    "page number :{page_number} \n",
    "\"\"\"\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "oembed = OllamaEmbeddings(base_url=\"http://localhost:11434\", model=\"nomic-embed-text\")\n",
    "collection_name=\"phydb\"  ##phydbllama3\n",
    "llm = Ollama(model=\"llama3.2\")\n",
    " \n",
    "persist_directory=\"database\"\n",
    "vectorstore = Chroma(\n",
    "     collection_name=collection_name,  \n",
    "    embedding_function=oembed,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "retriever = vectorstore.as_retriever()   \n",
    "system_prompt = (\n",
    "        \"You are an assistant for question-answering tasks. \"\n",
    "        \"Use the following pieces of retrieved context to answer \"\n",
    "        \"the question. If you don't know the answer, say that you \"\n",
    "        \"don't know. Use three sentences maximum and keep the \"\n",
    "        \"answer concise.\"\n",
    "        \"\\n\\n\"\n",
    "        \"{context}\"\n",
    "        \n",
    "    )\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "contextualize_q_system_prompt = (\n",
    "        \"Given a chat history and the latest user question \"\n",
    "        \"which might reference context in the chat history, \"\n",
    "        \"formulate a standalone question which can be understood \"\n",
    "        \"without the chat history. Do NOT answer the question, \"\n",
    "        \"just reformulate it if needed and otherwise return it as is.\"\n",
    "    )\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", contextualize_q_system_prompt),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "        llm, retriever, contextualize_q_prompt\n",
    "    )\n",
    "\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "message = \"list the uses of solar cell?\"\n",
    "history = []\n",
    "answer = rag_chain.invoke({\"input\": message, \"chat_history\": history})\n",
    "for q in answer[\"context\"]:\n",
    "    answer[\"answer\"]= answer[\"answer\"] +\" page : \"+ str(q.metadata['page']) +\"/n\"\n",
    "history.extend(\n",
    "        [\n",
    "            HumanMessage(content=message),\n",
    "            AIMessage(content=answer[\"answer\"]),\n",
    "        ]\n",
    "        )\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answer[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the provided context, kinetic energy is the capacity of doing work acquired by a moving body due to its motion. It depends on both the mass and velocity of an object, with increased kinetic energy resulting from greater mass or velocity. page : 76\n",
      " page : 77\n",
      " page : 76\n",
      " page : 108\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "message = \"what is kinetic energy?\"\n",
    "\n",
    "answer = rag_chain.invoke({\"input\": message, \"chat_history\": history})\n",
    "for q in answer[\"context\"]:\n",
    "    answer[\"answer\"]= answer[\"answer\"] +\" page : \"+ str(q.metadata['page']) +\"\\n\"\n",
    "history.extend(\n",
    "        [\n",
    "            HumanMessage(content=message),\n",
    "            AIMessage(content=answer[\"answer\"]),\n",
    "        ]\n",
    "        )\n",
    "print(answer[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page : [76, 77, 108]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "run = []\n",
    "for q in answer[\"context\"]:\n",
    "    if q.metadata['page'] not in run :\n",
    "    \n",
    "        run.append(q.metadata['page'])\n",
    "run.sort()   \n",
    "print(\"page : \" + str(run))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='list the uses of solar cell?' additional_kwargs={} response_metadata={}\n",
      "content='According to the context, solar cells have been used to supply electricity in artificial satellites (page 2) and also operate electronic devices like pocket calculators, radios, and watches.\\n\\nPage reference: Page 1-3' additional_kwargs={} response_metadata={}\n",
      "content='what is force?' additional_kwargs={} response_metadata={}\n",
      "content='According to the context, force refers to the consequence of direct contact between two bodies. It can be either a contact force, such as frictional or pulling force, or a non-contact force, like gravitational or magnetic force.\\n\\nPage reference: Page 1-3' additional_kwargs={} response_metadata={}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for q in answer[\"chat_history\"]:\n",
    "    print(q )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset \n",
    "data_samples = {\n",
    "    'question': [\n",
    "        'Who played a vital role in the Industrial Revolution with the invention of the steam engine?',\n",
    "        'Which scientist demonstrated the magnetic effect of current?',\n",
    "        'Who discovered that magnetic effect produces electric current?',\n",
    "        'Who demonstrated that light is one kind of electromagnetic wave?',\n",
    "        'Who discovered the method of sending signals through Morse code?',\n",
    "        'Who contributed to the development of radio communication by sending energy through electromagnetic waves?',\n",
    "        'Who discovered X-rays?',\n",
    "        'Who discovered the radioactivity of uranium?',\n",
    "        'Who discovered the quantum theory of radiation?',\n",
    "        'Who invented the theory of relativity?',\n",
    "        'What important discovery did Otto Hahn and Stresemann make in 1938?',\n",
    "        'What particle is named after Satyendranath Basu?',\n",
    "        'Who discovered the Raman effect?',\n",
    "        'Which concept was first presented by Euclid?',\n",
    "        'Which scientist applied space and time in the law of motion?',\n",
    "        'How did Newtonian mechanics describe space?'\n",
    "    ],\n",
    "    'ground_truth': [\n",
    "        'James Watt',\n",
    "        'Hans Christian Oersted',\n",
    "        'Michael Faraday, Henry, and Lenz',\n",
    "        'James Clark Maxwell',\n",
    "        'Marconi',\n",
    "        'Sir Jagadish Chandra Basu',\n",
    "        'Roentgen',\n",
    "        'Becquerel',\n",
    "        'Max Planck',\n",
    "        'Albert Einstein',\n",
    "        'They discovered that the nucleus was fissionable.',\n",
    "        'Boson',\n",
    "        'Chandra Shekhar Raman',\n",
    "        'The geometric concept of space.',\n",
    "        'Galileo',\n",
    "        'As a three-dimensional extension that is continuous and homogenous.'\n",
    "    ],\n",
    "    'contexts': [\n",
    "        [\n",
    "            'The discovery and inventions of the eighteenth and nineteenth century paved the way for Europe to industrial revolution. The steam engine of James Watt (1736-1819 A.D) played a vital role for industrial revolution.'\n",
    "        ],\n",
    "        [\n",
    "            'Hans Christian Oersted (1777-1851 A.D) demonstrated the magnetic effect of current.'\n",
    "        ],\n",
    "        [\n",
    "            'This discovery led Michael Faraday (1791-1867 A.D), Henry (1797-1879 A.D) and Lenz (1804-1865 A.D) towards discovering the fact that magnetic effect produces electric current.'\n",
    "        ],\n",
    "        [\n",
    "            'In 1864 James Clark Maxwell (1831-1879 A.D) demonstrated that light is one kind of electromagnetic wave.'\n",
    "        ],\n",
    "        [\n",
    "            'Using the same kind of waves in 1896, Marconi (1874-1937 A.D) discovered the method of sending signal through Morse code to far off distance.'\n",
    "        ],\n",
    "        [\n",
    "            'Before him Sir Jagadish Chandra Basu (1858-1937 A.D) was able to send energy from one place to another through electromagnetic wave. In this way radio communication was developed.'\n",
    "        ],\n",
    "        [\n",
    "            'By the end of nineteenth century Roentgen (1845-1923 A.D) discovered x-rays.'\n",
    "        ],\n",
    "        [\n",
    "            'Becquerel (1852-1908 A.D) discovered the radioactivity of uranium.'\n",
    "        ],\n",
    "        [\n",
    "            'Max Planck (1858-1947 A.D) discovered quantum theory of radiation.'\n",
    "        ],\n",
    "        [\n",
    "            'Albert Einstein (1879-1955 A.D) invented theory of relativity.'\n",
    "        ],\n",
    "        [\n",
    "            'In 1938 Otto Hahn (1879-1968 A.D) and Stresemann (1902-1980 A.D) found out that the nucleus was fissionable.'\n",
    "        ],\n",
    "        [\n",
    "            'Satyendranath Basu (1894-1974 A.D) professor of physics, University of Dhaka made important contribution on theoretical physics. As recognition of his contribution one kind of elementary particle is named after him and is called Boson.'\n",
    "        ],\n",
    "        [\n",
    "            'Prior to that nobel laureate physicist Chandra Shekhar Ramon (1888-1970 A.D) discovered Raman effect.'\n",
    "        ],\n",
    "        [\n",
    "            'Euclid was the first to present the geometric concept of space.'\n",
    "        ],\n",
    "        [\n",
    "            'Galileo in his book, Statics used space and time in the law of motion and acceleration.'\n",
    "        ],\n",
    "        [\n",
    "            'Newtonian mechanics described space as a three-dimensional extension. Space is continuous, homogenous, and independent.'\n",
    "        ]\n",
    "    ],\n",
    "    \n",
    "}\n",
    "\n",
    "eval_set = Dataset.from_dict(data_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_20020\\2134469951.py:18: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 0.4. An updated version of the class exists in the langchain-chroma package and should be used instead. To use it run `pip install -U langchain-chroma` and import as `from langchain_chroma import Chroma`.\n",
      "  vectorstore = Chroma(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "oembed = OllamaEmbeddings(base_url=\"http://localhost:11434\", model=\"nomic-embed-text\")\n",
    "collection_name=\"phydb\"  ##phydbllama3\n",
    "llm = Ollama(model=\"llama3.2\")\n",
    " \n",
    "persist_directory=\"database\"\n",
    "vectorstore = Chroma(\n",
    "     collection_name=collection_name,  \n",
    "    embedding_function=oembed,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "retriever = vectorstore.as_retriever()   \n",
    "system_prompt = (\n",
    "        \"You are an assistant for question-answering tasks. \"\n",
    "        \"Use the following pieces of retrieved context to answer \"\n",
    "        \"the question. If you don't know the answer, say that you \"\n",
    "        \"don't know. Use three sentences maximum and keep the \"\n",
    "        \"answer concise.\"\n",
    "        \"\\n\\n\"\n",
    "        \"{context}\"\n",
    "        \n",
    "    )\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "contextualize_q_system_prompt = (\n",
    "        \"Given a chat history and the latest user question \"\n",
    "        \"which might reference context in the chat history, \"\n",
    "        \"formulate a standalone question which can be understood \"\n",
    "        \"without the chat history. Do NOT answer the question, \"\n",
    "        \"just reformulate it if needed and otherwise return it as is.\"\n",
    "    )\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", contextualize_q_system_prompt),\n",
    "            \n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "        llm, retriever, contextualize_q_prompt\n",
    "    )\n",
    "\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "           \n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    "    \n",
    "    context_utilization,\n",
    "    answer_correctness,\n",
    "    answer_similarity\n",
    ")\n",
    "from ragas.run_config import RunConfig\n",
    "from ragas.metrics.critique import harmfulness\n",
    "from ragas import evaluate\n",
    "run_config = RunConfig(timeout=120.0) \n",
    "def create_ragas_dataset(rag_chain, eval_dataset):\n",
    "  rag_dataset = []\n",
    "  for row in tqdm(eval_dataset):\n",
    "    answer = rag_chain.invoke({\"input\" : row[\"question\"]})\n",
    "    rag_dataset.append(\n",
    "        {\"question\" : row[\"question\"],\n",
    "         \"answer\" : answer['answer'],\n",
    "         \"contexts\" : [context.page_content for context in answer[\"context\"]],\n",
    "         \"ground_truths\" : [row[\"ground_truth\"]]\n",
    "         }\n",
    "    )\n",
    "  rag_df = pd.DataFrame(rag_dataset)\n",
    "  rag_eval_dataset = Dataset.from_pandas(rag_df)\n",
    "  return rag_eval_dataset\n",
    "\n",
    "def evaluate_ragas_dataset(ragas_dataset):\n",
    "  result = evaluate(\n",
    "    ragas_dataset,\n",
    "    metrics=[\n",
    "        context_utilization,\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        context_recall,\n",
    "        answer_correctness,\n",
    "        answer_similarity\n",
    "    ],\n",
    "     llm=llm, \n",
    "     embeddings=oembed,\n",
    "  )\n",
    "  return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [03:53<00:00, 14.61s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "basic_qa_ragas_dataset = create_ragas_dataset(rag_chain, eval_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "path ='dataset/fact/factos.json'\n",
    "with open(path) as f:\n",
    "  json_data = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##set-local-model --model-name=llama-3.2-90b-text-preview --base-url=\"https://api.groq.com/openai/v1\" \n",
    "# --api-key=\"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\"\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.metrics import HallucinationMetric\n",
    "import time as t\n",
    "from tqdm import tqdm\n",
    "a=[]\n",
    "for i in tqdm(json_data):\n",
    "    message = i['question']\n",
    "    test_case = LLMTestCase(\n",
    "        input=i['question'],\n",
    "        actual_output=i['answer'],\n",
    "        context=[i['context']]\n",
    "    )\n",
    "\n",
    "    metric = HallucinationMetric(threshold=0.5)\n",
    "    try:\n",
    "        metric.measure(test_case)\n",
    "        a.append(metric.score)\n",
    "        t.sleep(5)\n",
    "    \n",
    "    except Exception as e:\n",
    "            print(f\"Unexpected error: {e}\")\n",
    "    \n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##set-local-model --model-name=Meta-Llama-3.1-405B-Instruct --base-url=\"https://api.sambanova.ai/v1\" --api-key=\"72d97fa6-b98f-470d-87d3-7f7a5e289df7\"\n",
    "from deepeval.metrics import HallucinationMetric\n",
    "from deepeval.metrics import ContextualPrecisionMetric\n",
    "from deepeval.metrics import ContextualRecallMetric\n",
    "from deepeval.metrics import ContextualRelevancyMetric\n",
    "from deepeval.metrics import KnowledgeRetentionMetric\n",
    "from deepeval.test_case import ConversationalTestCase\n",
    "from deepeval.metrics import GEval\n",
    "from deepeval.test_case import LLMTestCaseParams\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "def ContextRelevancyMetric(input,actual_output,retrieval_context):\n",
    "        test = LLMTestCase(\n",
    "        input=input,\n",
    "        actual_output=actual_output,\n",
    "        retrieval_context=retrieval_context\n",
    "        )\n",
    "        metric = ContextualRelevancyMetric(threshold=0.8)\n",
    "        arr = None\n",
    "        try:\n",
    "            metric.measure(test)\n",
    "            arr = {\n",
    "\n",
    "                \"score\": metric.score,\n",
    "                \"reason\": metric.reason\n",
    "            }\n",
    "            print(f\"ContextRelevancyMetric: {metric.score}\")\n",
    "            # print(metric.reason)\n",
    "        except ValueError as ve:\n",
    "            print(f\"ValueError: {ve}\")\n",
    "            # print(f\"Test Case: {test}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error: {e}\")\n",
    "\n",
    "        return arr\n",
    "\n",
    "def ContextRecallMetric(input,actual_output,expected_output,retrieval_context):\n",
    "        test = LLMTestCase(\n",
    "        input=input,\n",
    "        actual_output=actual_output,\n",
    "        expected_output=expected_output,\n",
    "        retrieval_context=retrieval_context\n",
    "        )\n",
    "        metric = ContextualRecallMetric(threshold=0.7)\n",
    "        arr = None\n",
    "        try:\n",
    "            metric.measure(test)\n",
    "            arr ={\n",
    "\n",
    "                \"score\": metric.score,\n",
    "                \"reason\": metric.reason\n",
    "            }\n",
    "            print(f\"ContextRecallMetric: {metric.score}\")\n",
    "        except ValueError as ve:\n",
    "            print(f\"ValueError: {ve}\")\n",
    "            # print(f\"Test Case: {test}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error: {e}\")\n",
    "\n",
    "        return arr\n",
    "\n",
    "def ContextPrecisionMetric(input,actual_output,expected_output,retrieval_context):\n",
    "        test = LLMTestCase(\n",
    "        input=input,\n",
    "        actual_output=actual_output,\n",
    "        expected_output=expected_output,\n",
    "        retrieval_context=retrieval_context\n",
    "        )\n",
    "        metric = ContextualPrecisionMetric(threshold=0.5)\n",
    "        arr = None\n",
    "        try:\n",
    "            metric.measure(test)\n",
    "            arr = {\n",
    "                \"score\": metric.score,\n",
    "                \"reason\": metric.reason\n",
    "            }\n",
    "            print(f\"ContextPrecisionMetric: {metric.score}\")\n",
    "        except ValueError as ve:\n",
    "            print(f\"ValueError: {ve}\")\n",
    "            # print(f\"Test Case: {test}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error: {e}\")\n",
    "\n",
    "        return arr\n",
    "\n",
    "def HallucinateMetrics(input,actual_output,context):\n",
    "        test = LLMTestCase(\n",
    "        input=input,\n",
    "        actual_output=actual_output,\n",
    "        context=context,\n",
    "        )\n",
    "        metric = HallucinationMetric(threshold=0.5)\n",
    "        arr = None\n",
    "        try:\n",
    "            metric.measure(test)\n",
    "\n",
    "            arr = {\n",
    "\n",
    "                \"score\": metric.score,\n",
    "                \"reason\": metric.reason\n",
    "            }\n",
    "            print(f\"HallucinateMetrics: {metric.score}\")\n",
    "        except ValueError as ve:\n",
    "            print(f\"ValueError: {ve}\")\n",
    "            print(f\"Test Case: {test}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error: {e}\")\n",
    "\n",
    "        return arr\n",
    "\n",
    "def KnowledgeMetrics(data):\n",
    "        turns = []\n",
    "        for d in data:\n",
    "        # Retrieve necessary fields\n",
    "            actual_output = d.get(\"answer\")\n",
    "            input = d.get(\"question\")\n",
    "\n",
    "            schema = {\n",
    "                \"question\":input,\n",
    "                \"answer\":actual_output,\n",
    "                \"metrics\": {\n",
    "                    \"KnowledgeRetention\":[],\n",
    "\n",
    "                }\n",
    "                    \n",
    "                \n",
    "            }\n",
    "\n",
    "            test = LLMTestCase(\n",
    "            input=input,\n",
    "            actual_output=actual_output,\n",
    "            \n",
    "            )\n",
    "            turns.append(test)\n",
    "        test_case = ConversationalTestCase(turns=turns)\n",
    "        metric = KnowledgeRetentionMetric(threshold=0.7)\n",
    "\n",
    "        arr = None\n",
    "        try:\n",
    "            metric.measure(test_case)\n",
    "            arr = {\n",
    "\n",
    "                \"score\": metric.score,\n",
    "                \"reason\": metric.reason\n",
    "            }\n",
    "            print(metric.score)\n",
    "            print(metric.reason)\n",
    "        except ValueError as ve:\n",
    "            print(f\"ValueError: {ve}\")\n",
    "            print(f\"Test Case: {test}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error: {e}\")\n",
    "        schema[\"metrics\"][\"KnowledgeRetention\"].append(arr)\n",
    "        return  schema\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# test_case_results.append(ComprehensionMetrics(data))\n",
    "import json\n",
    "def runAllTests(data,n):\n",
    "    test_case_results = []\n",
    "    for i, d in enumerate(data):\n",
    "        # Retrieve necessary fields\n",
    "        actual_output = d.get(\"answer\")\n",
    "        input = d.get(\"question\")\n",
    "        expected_output = d[\"referenceAnswer\"]\n",
    "        \n",
    "        context =[ d[\"referenceContext\"] ]\n",
    "        retrieval_context = [d.get(\"context\")]\n",
    "        schema = {\n",
    "            \"question\":input,\n",
    "            \"answer\":actual_output,\n",
    "            \"referenceAnswer\":expected_output,\n",
    "            \"referenceContext\":retrieval_context,\n",
    "            \"context\": context,\n",
    "            \"metrics\": {\n",
    "                \"Hallucination\":[],\n",
    "                \"ContextPrecision\": [],\n",
    "                \"ContextRecall\": [],\n",
    "                \"ContextRelevancy\": []\n",
    "            }\n",
    "                \n",
    "            \n",
    "        }\n",
    "        schema[\"metrics\"][\"ContextPrecision\"].append(ContextPrecisionMetric(input,actual_output,expected_output,retrieval_context))\n",
    "        schema[\"metrics\"][\"ContextRecall\"].append(ContextRecallMetric(input,actual_output,expected_output,retrieval_context))\n",
    "        schema[\"metrics\"][\"ContextRelevancy\"].append(ContextRelevancyMetric(input,actual_output,retrieval_context))\n",
    "        schema[\"metrics\"][\"Hallucination\"].append(HallucinateMetrics(input,actual_output,context))\n",
    "        test_case_results.append(schema)\n",
    "        print(f\"Dataset: {i}\")\n",
    "        # print(schema)\n",
    "        # Print debug information\n",
    "        # print(f\"Input: {input}, Actual Output: {actual_output}, Context: {context}\")\n",
    "        with open(f\"dataset/dataset_evaluated_{n}.json\", 'a') as f:\n",
    "          json.dump(schema, f, indent=4)\n",
    "        print(\"Saved Successfully!\")\n",
    "        # Create the test case\n",
    "    return test_case_results\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "\n",
    "with open(f\"dataset/conc4to14and5and11.json\", \"r\", encoding=\"utf-8\") as json_file:\n",
    "        data = json.load(json_file)\n",
    "data = data[104:]\n",
    "\n",
    "    # Save evaluation data\n",
    "test_case_results = runAllTests(data,1)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"evaluatedMetrics/all_eval.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Hallucination': 33, 'ContextPrecision': 80, 'ContextRecall': 93, 'Relevance': 28}\n"
     ]
    }
   ],
   "source": [
    "# arr = []\n",
    "import json\n",
    "# for i in [1,2,3,4,5,6,8,9,10,11,12,13,14]:\n",
    "with open(f\"dataset\\dataset_evaluated_1.json\", \"r\", encoding=\"utf-8\") as json_file:\n",
    "        data = json.load(json_file)\n",
    "#     for d in data:\n",
    "#         arr.append(d)\n",
    "# with open(f\"evaluatedMetrics/all_eval.json.json\", 'a') as f:\n",
    "#      json.dump(arr, f, indent=4)\n",
    "# print(len(arr))\n",
    "def Calculate(data):\n",
    "    Hallucinate = 0\n",
    "    Hcount = 0\n",
    "    ContextPrecision = 0\n",
    "    CPcount = 0\n",
    "    ContextRecall = 0\n",
    "    CRcount = 0\n",
    "    ContextRelevancy = 0\n",
    "    CRVcount = 0\n",
    "    \n",
    "    for d in data:\n",
    "        try:\n",
    "            # Accessing nested scores directly using indexing\n",
    "            hallucination = d[\"metrics\"][\"Hallucination\"][0][\"score\"]\n",
    "            Hallucinate += hallucination\n",
    "            Hcount += 1\n",
    "        except (KeyError, IndexError, TypeError):\n",
    "            pass  # Safely skip if any key or index is missing\n",
    "\n",
    "        try:\n",
    "            precision = d[\"metrics\"][\"ContextPrecision\"][0][\"score\"]\n",
    "            ContextPrecision += precision\n",
    "            CPcount += 1\n",
    "        except (KeyError, IndexError, TypeError):\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            recall = d[\"metrics\"][\"ContextRecall\"][0][\"score\"]\n",
    "            ContextRecall += recall\n",
    "            CRcount += 1\n",
    "        except (KeyError, IndexError, TypeError):\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            relevancy = d[\"metrics\"][\"ContextRelevancy\"][0][\"score\"]\n",
    "            ContextRelevancy += relevancy\n",
    "            CRVcount += 1\n",
    "        except (KeyError, IndexError, TypeError):\n",
    "            pass\n",
    "    Havg = int((Hallucinate / Hcount) * 100) if Hcount > 0 else 0\n",
    "    CPavg = (ContextPrecision / CPcount) * 100 if CPcount > 0 else 0\n",
    "    CRavg = (ContextRecall / CRcount) * 100 if CRcount > 0 else 0\n",
    "    CPavg = int(CPavg)\n",
    "    CRavg = int(CRavg)\n",
    "    RetrievalPrecision = int((CRavg + CPavg) / 2) if CPcount > 0 and CRcount > 0 else 0\n",
    "    CRVavg = int((ContextRelevancy / CRVcount) * 100) if CRVcount > 0 else 0\n",
    "    return { \n",
    "         \"Hallucination\":Havg,\n",
    "         \"ContextPrecision\":CPavg,\n",
    "         \"ContextRecall\":CRavg,\n",
    "        #  \"RetrievalPrecision\":RetrievalPrecision,\n",
    "         \"Relevance\":CRVavg\n",
    "    }\n",
    "\n",
    "print(Calculate(data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
